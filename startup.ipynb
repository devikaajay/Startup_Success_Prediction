{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devikaajay/Startup_Success_Prediction/blob/main/startup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQUEp4WmseSr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.impute import SimpleImputer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYCeR88K0IXl",
        "outputId": "5cd56dc7-5366-4b89-e7c7-4e6edff47622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape after column selection: (66368, 10)\n",
            "Shape after removing all-column duplicates: (66367, 10)\n",
            "Shape after enforcing unique company names: (66103, 10)\n",
            "Shape after removing all rows with null values: (52551, 10)\n",
            "\n",
            "First 5 rows of the cleaned data:\n",
            "                     name                       homepage_url  \\\n",
            "0                   #fame                 http://livfame.com   \n",
            "1                :Qounter             http://www.qounter.com   \n",
            "3                 0-6.com                 http://www.0-6.com   \n",
            "4        004 Technologies  http://004gmbh.de/en/004-interact   \n",
            "6  Ondine Biomedical Inc.               http://ondinebio.com   \n",
            "\n",
            "                                       category_list funding_total_usd  \\\n",
            "0                                              Media          10000000   \n",
            "1  Application Platforms|Real Time|Social Network...            700000   \n",
            "3                                        Curated Web           2000000   \n",
            "4                                           Software                 -   \n",
            "6                                      Biotechnology            762851   \n",
            "\n",
            "      status country_code state_code                 region           city  \\\n",
            "0  operating          IND         16                 Mumbai         Mumbai   \n",
            "1  operating          USA         DE             DE - Other  Delaware City   \n",
            "3  operating          CHN         22                Beijing        Beijing   \n",
            "4  operating          USA         IL  Springfield, Illinois      Champaign   \n",
            "6  operating          CAN         BC              Vancouver      Vancouver   \n",
            "\n",
            "   funding_rounds  \n",
            "0               1  \n",
            "1               2  \n",
            "3               1  \n",
            "4               1  \n",
            "6               2  \n",
            "\n",
            "Cleaned data saved to: cleaned_startup_data_for_prediction.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the CSV file\n",
        "try:\n",
        "    df = pd.read_csv('/content/big_startup_secsees_dataset.csv', encoding='utf-8')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'big_startup_secsees_dataset.csv' not found. Please upload your file and ensure the name is correct.\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "REQUIRED_COLUMNS = [\n",
        "    'name',\n",
        "    'homepage_url',\n",
        "    'category_list',\n",
        "    'funding_total_usd',\n",
        "    'status',\n",
        "    'country_code',\n",
        "    'state_code',\n",
        "    'region',\n",
        "    'city',\n",
        "    'funding_rounds'\n",
        "]\n",
        "\n",
        "# --- Data Cleaning and Selection ---\n",
        "\n",
        "if df.empty:\n",
        "    print(\"DataFrame is empty. Skipping further processing.\")\n",
        "else:\n",
        "\n",
        "    missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Error: The following required columns are missing from the uploaded file: {', '.join(missing_cols)}\")\n",
        "\n",
        "        df = df[[col for col in REQUIRED_COLUMNS if col in df.columns]]\n",
        "    else:\n",
        "        df = df[REQUIRED_COLUMNS]\n",
        "\n",
        "\n",
        "    print(f\"Original shape after column selection: {df.shape}\")\n",
        "\n",
        "    df_no_all_duplicates = df.drop_duplicates()\n",
        "    print(f\"Shape after removing all-column duplicates: {df_no_all_duplicates.shape}\")\n",
        "\n",
        "    df_unique_names = df_no_all_duplicates.drop_duplicates(subset=['name'], keep='first')\n",
        "    print(f\"Shape after enforcing unique company names: {df_unique_names.shape}\")\n",
        "\n",
        "    df_cleaned = df_unique_names.dropna()\n",
        "    print(f\"Shape after removing all rows with null values: {df_cleaned.shape}\")\n",
        "\n",
        "\n",
        "    print(\"\\nFirst 5 rows of the cleaned data:\")\n",
        "    print(df_cleaned.head())\n",
        "\n",
        "    CLEANED_FILE_NAME = 'cleaned_startup_data_for_prediction.csv'\n",
        "    df_cleaned.to_csv(CLEANED_FILE_NAME, index=False)\n",
        "    print(f\"\\nCleaned data saved to: {CLEANED_FILE_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCs49cXGuOXv",
        "outputId": "b3442c21-3f2f-4a13-c3f0-001234963261"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52551, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D5sACeF1vYA",
        "outputId": "709c9637-68cc-47d9-fc51-35ecf2ab3799",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting shape for advanced cleaning: (52551, 10)\n",
            "Shape after removing names starting with non-letters: (52093, 10) (458 rows removed)\n",
            "Shape after removing numeric state codes: (39606, 10) (12487 rows removed)\n",
            "Shape after removing single-letter names: (39606, 10) (0 rows removed)\n",
            "Shape after validating/cleaning funding values: (34069, 10) (5537 rows removed)\n",
            "Dataset truncated to the top 3000 rows.\n",
            "\n",
            "--- Final Cleaned Data Summary ---\n",
            "Final shape of the super-cleaned DataFrame: (3000, 10)\n",
            "\n",
            "First 5 rows of the super-cleaned data:\n",
            "| name                   | homepage_url               | category_list                                            | funding_total_usd   | status    | country_code   | state_code   | region      | city          | funding_rounds   |\n",
            "|:-----------------------|:---------------------------|:---------------------------------------------------------|:--------------------|:----------|:---------------|:-------------|:------------|:--------------|:-----------------|\n",
            "| Ondine Biomedical Inc. | http://ondinebio.com       | Biotechnology                                            | 762851              | operating | CAN            | BC           | Vancouver   | Vancouver     | 2                |\n",
            "| H2O.ai                 | http://h2o.ai/             | Analytics                                                | 3.36e+07            | operating | USA            | CA           | SF Bay Area | Mountain View | 4                |\n",
            "| One Inc.               | http://whatis1.com         | Mobile                                                   | 1.15005e+06         | operating | USA            | CA           | SF Bay Area | San Francisco | 3                |\n",
            "| ZenChef                | http://zenchef.com/en/     | Local Businesses|Restaurants                             | 1.06618e+07         | operating | FRA            | A8           | Paris       | Paris         | 5                |\n",
            "| Redox                  | http://www.redoxengine.com | Health Care|Health Care Information Technology|Hospitals | 4e+06               | operating | USA            | WI           | Madison     | Madison       | 2                |\n",
            "\n",
            "Super-cleaned data saved to: super_cleaned_startup_data_for_prediction.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Starting shape for advanced cleaning: {df_cleaned.shape}\")\n",
        "\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "name_mask = df_cleaned['name'].astype(str).str.match(r'^[a-zA-Z]')\n",
        "df_cleaned = df_cleaned[name_mask]\n",
        "print(f\"Shape after removing names starting with non-letters: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "numeric_state_mask = df_cleaned['state_code'].astype(str).str.isnumeric()\n",
        "\n",
        "df_cleaned = df_cleaned[~numeric_state_mask]\n",
        "print(f\"Shape after removing numeric state codes: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "single_letter_mask = df_cleaned['name'].astype(str).str.len() == 1\n",
        "\n",
        "df_cleaned = df_cleaned[~single_letter_mask]\n",
        "print(f\"Shape after removing single-letter names: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "df_cleaned['funding_total_usd'] = pd.to_numeric(\n",
        "    df_cleaned['funding_total_usd'],\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "df_cleaned.dropna(subset=['funding_total_usd'], inplace=True)\n",
        "print(f\"Shape after validating/cleaning funding values: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "TARGET_ROWS = 3000\n",
        "initial_shape_trunc = df_cleaned.shape[0]\n",
        "\n",
        "if initial_shape_trunc > TARGET_ROWS:\n",
        "    df_cleaned = df_cleaned.head(TARGET_ROWS)\n",
        "    print(f\"Dataset truncated to the top {TARGET_ROWS} rows.\")\n",
        "elif initial_shape_trunc < TARGET_ROWS:\n",
        "    print(f\"Warning: Only {initial_shape_trunc} rows remain after cleaning. No truncation performed.\")\n",
        "else:\n",
        "    print(f\"Dataset size is exactly {TARGET_ROWS} rows after cleaning. No truncation needed.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Cleaned Data Summary ---\")\n",
        "print(f\"Final shape of the super-cleaned DataFrame: {df_cleaned.shape}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of the super-cleaned data:\")\n",
        "print(df_cleaned.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "SUPER_CLEANED_FILE_NAME = 'super_cleaned_startup_data_for_prediction.csv'\n",
        "df_cleaned.to_csv(SUPER_CLEANED_FILE_NAME, index=False)\n",
        "print(f\"\\nSuper-cleaned data saved to: {SUPER_CLEANED_FILE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn_N1qZ_3yde",
        "outputId": "ef221075-15aa-48bf-f522-eb7a9e693a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OeZVa1I6wX3",
        "outputId": "98288a6e-f102-4ea8-8e67-c243dcd8bd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting web scraping process using HTML parsing for the CB Insights list.\n",
            "NOTE: Funding amounts will be converted to INR using a rate of 1 USD = 88.78 INR.\n",
            "Attempting to fetch HTML and parse the main company table from: https://www.cbinsights.com/research-unicorn-companies...\n",
            "SUCCESS: Found 1303 company rows. This might be the full list or the initial static content.\n",
            "Found 3000 existing records in super_cleaned_startup_data_for_prediction.csv.\n",
            "\n",
            "--- Scraping Complete ---\n",
            "Successfully scraped 1303 new/updated records.\n",
            "Combined and saved 4279 unique records (total) to super_cleaned_startup_data_for_prediction.csv.\n",
            "Total duplicates removed/overwritten: 24.\n",
            "\n",
            "--- Sample of FINAL Saved Data (Funding in INR) ---\n",
            "| name                | homepage_url               | category_list   |   funding_total_usd | status    | country_code   | state_code            | region                | city                  |   funding_rounds |\n",
            "|:--------------------|:---------------------------|:----------------|--------------------:|:----------|:---------------|:----------------------|:----------------------|:----------------------|-----------------:|\n",
            "| LeadSquared         | www.leadsquared.com        | India           |               88.78 | operating | Bengaluru      | Enterprise Tech       | Enterprise Tech       | Enterprise Tech       |                5 |\n",
            "| FourKites           | www.fourkites.com          | United States   |               88.78 | operating | Chicago        | Enterprise Tech       | Enterprise Tech       | Enterprise Tech       |                5 |\n",
            "| VulcanForms         | www.vulcanforms.com        | United States   |               88.78 | operating | Burlington     | Industrials           | Industrials           | Industrials           |                5 |\n",
            "| SingleStore         | www.singlestore.com        | United States   |               88.78 | operating | San Francisco  | Enterprise Tech       | Enterprise Tech       | Enterprise Tech       |                5 |\n",
            "| Unstoppable Domains | www.unstoppabledomains.com | United States   |               88.78 | operating | Las Vegas      | Media & Entertainment | Media & Entertainment | Media & Entertainment |                5 |\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "TARGET_URL = 'https://www.cbinsights.com/research-unicorn-companies'\n",
        "SCRAPED_FILE_NAME = 'super_cleaned_startup_data_for_prediction.csv'\n",
        "\n",
        "\n",
        "INR_CONVERSION_RATE = 88.78\n",
        "\n",
        "SCRAPED_COLUMNS = [\n",
        "    'name',\n",
        "    'homepage_url',\n",
        "    'category_list',\n",
        "    'funding_total_usd',\n",
        "    'status',\n",
        "    'country_code',\n",
        "    'state_code',\n",
        "    'region',\n",
        "    'city',\n",
        "    'funding_rounds'\n",
        "]\n",
        "\n",
        "scraped_data = []\n",
        "\n",
        "def clean_funding_value(funding_str):\n",
        "    \"\"\"\n",
        "    Converts money strings (e.g., '$1.2B', '$500M') to numerical INR.\n",
        "    The final value is multiplied by the INR_CONVERSION_RATE.\n",
        "    \"\"\"\n",
        "    if not isinstance(funding_str, str):\n",
        "        return None\n",
        "\n",
        "    text = funding_str.strip().replace('$', '').replace(',', '').upper()\n",
        "\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    usd_value = None\n",
        "\n",
        "    if 'B' in text:\n",
        "        try:\n",
        "            usd_value = float(text.replace('B', '').strip()) * 1_000_000_000\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    elif 'M' in text:\n",
        "        try:\n",
        "            usd_value = float(text.replace('M', '').strip()) * 1_000_000\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    elif 'K' in text:\n",
        "        try:\n",
        "            usd_value = float(text.replace('K', '').strip()) * 1_000\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            usd_value = float(text)\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    if usd_value is not None:\n",
        "\n",
        "        return usd_value * INR_CONVERSION_RATE\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def scrape_data(url):\n",
        "    \"\"\"\n",
        "    Fetches the HTML page and scrapes the main table elements directly.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to fetch HTML and parse the main company table from: {url}...\")\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "        'Referer': 'https://www.google.com/',\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "        main_content_div = soup.find('div', id='company_list_table')\n",
        "        if not main_content_div:\n",
        "             main_content_div = soup\n",
        "\n",
        "        table = main_content_div.find('table')\n",
        "\n",
        "        if not table:\n",
        "            print(\"ERROR: Could not find the main company table using generic selectors.\")\n",
        "            return []\n",
        "\n",
        "        tbody = table.find('tbody')\n",
        "        if not tbody:\n",
        "            print(\"ERROR: Found table but could not locate the tbody for rows.\")\n",
        "            return []\n",
        "\n",
        "        rows = tbody.find_all('tr')\n",
        "\n",
        "        if not rows:\n",
        "            print(\"ERROR: Found the table but no company rows were present.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"SUCCESS: Found {len(rows)} company rows. This might be the full list or the initial static content.\")\n",
        "\n",
        "        page_results = []\n",
        "        for i, row in enumerate(rows):\n",
        "\n",
        "            cols = row.find_all(['td', 'th'])\n",
        "\n",
        "            if len(cols) < 7:\n",
        "                continue\n",
        "            name_cell = cols[0].find('a')\n",
        "            company_name = name_cell.text.strip() if name_cell else cols[0].text.strip()\n",
        "\n",
        "\n",
        "            valuation_raw = cols[1].text.strip()\n",
        "\n",
        "            category_list_raw = cols[3].text.strip()\n",
        "\n",
        "            country_raw = cols[4].text.strip()\n",
        "\n",
        "            city_raw = cols[5].text.strip()\n",
        "\n",
        "            sanitized_name = re.sub(r'[^a-z0-9]', '', company_name.lower())\n",
        "            homepage_url_derived = f\"www.{sanitized_name}.com\"\n",
        "            unicorn_info = {\n",
        "                'name': company_name,\n",
        "                'homepage_url': homepage_url_derived,\n",
        "                'category_list': category_list_raw,\n",
        "                'funding_total_usd': clean_funding_value(valuation_raw),\n",
        "                'status': 'operating',\n",
        "                'country_code': country_raw,\n",
        "                'state_code': city_raw,\n",
        "                'region': city_raw,\n",
        "                'city': city_raw,\n",
        "                'funding_rounds': 5\n",
        "            }\n",
        "            page_results.append(unicorn_info)\n",
        "\n",
        "        return page_results\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"ERROR: HTTP error {response.status_code} fetching HTML data {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "print(f\"Starting web scraping process using HTML parsing for the CB Insights list.\")\n",
        "print(f\"NOTE: Funding amounts will be converted to INR using a rate of 1 USD = {INR_CONVERSION_RATE} INR.\")\n",
        "\n",
        "scraped_data = scrape_data(TARGET_URL)\n",
        "\n",
        "if scraped_data:\n",
        "    df_new = pd.DataFrame(scraped_data, columns=SCRAPED_COLUMNS)\n",
        "    df_new.replace({np.nan: None, '': None}, inplace=True)\n",
        "\n",
        "\n",
        "    df_existing = pd.DataFrame()\n",
        "\n",
        "    if os.path.exists(SCRAPED_FILE_NAME):\n",
        "        try:\n",
        "\n",
        "            df_existing = pd.read_csv(SCRAPED_FILE_NAME)\n",
        "            print(f\"Found {len(df_existing)} existing records in {SCRAPED_FILE_NAME}.\")\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(\"Existing file found but it was empty.\")\n",
        "        except Exception as e:\n",
        "\n",
        "            print(f\"Error reading existing file: {e}. Starting with only new data.\")\n",
        "\n",
        "    df_existing = df_existing[SCRAPED_COLUMNS] if not df_existing.empty else df_existing\n",
        "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "    initial_total = len(df_combined)\n",
        "\n",
        "    df_combined.drop_duplicates(subset=['name'], keep='last', inplace=True)\n",
        "    final_total = len(df_combined)\n",
        "\n",
        "    df_combined.to_csv(SCRAPED_FILE_NAME, index=False)\n",
        "\n",
        "    print(\"\\n--- Scraping Complete ---\")\n",
        "\n",
        "    print(f\"Successfully scraped {len(df_new)} new/updated records.\")\n",
        "    print(f\"Combined and saved {final_total} unique records (total) to {SCRAPED_FILE_NAME}.\")\n",
        "    print(f\"Total duplicates removed/overwritten: {initial_total - final_total}.\")\n",
        "\n",
        "    print(\"\\n--- Sample of FINAL Saved Data (Funding in INR) ---\")\n",
        "    print(df_combined.tail().to_markdown(index=False))\n",
        "\n",
        "else:\n",
        "    print(\"No data was scraped. The website likely loads the table content using JavaScript that our script cannot execute. The manual copy-paste method remains the only guaranteed alternative.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnqqhOUXXTgN",
        "outputId": "d7435b11-8f94-4734-b99e-2f6dc8cccc89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4279, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "df_combined.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnJoEWnyUwUr",
        "outputId": "d158354e-73b9-48f5-dd5e-0701cd84229f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting shape for advanced cleaning: (4279, 10)\n",
            "Shape after removing names starting with non-letters: (4268, 10) (11 rows removed)\n",
            "Shape after removing numeric state codes: (4268, 10) (0 rows removed)\n",
            "Shape after removing single-letter names: (4268, 10) (0 rows removed)\n",
            "Shape after validating/cleaning funding values: (4268, 10) (0 rows removed)\n",
            "Dataset truncated to the top 3000 rows.\n",
            "\n",
            "--- Final Cleaned Data Summary ---\n",
            "Final shape of the super-cleaned DataFrame: (3000, 10)\n",
            "\n",
            "First 5 rows of the super-cleaned data:\n",
            "| name                          | homepage_url               | category_list                                            | funding_total_usd   | status    | country_code   | state_code   | region      | city          | funding_rounds   |\n",
            "|:------------------------------|:---------------------------|:---------------------------------------------------------|:--------------------|:----------|:---------------|:-------------|:------------|:--------------|:-----------------|\n",
            "| Ondine Biomedical Inc.        | http://ondinebio.com       | Biotechnology                                            | 762851              | operating | CAN            | BC           | Vancouver   | Vancouver     | 2                |\n",
            "| One Inc.                      | http://whatis1.com         | Mobile                                                   | 1.15005e+06         | operating | USA            | CA           | SF Bay Area | San Francisco | 3                |\n",
            "| ZenChef                       | http://zenchef.com/en/     | Local Businesses|Restaurants                             | 1.06618e+07         | operating | FRA            | A8           | Paris       | Paris         | 5                |\n",
            "| Redox                         | http://www.redoxengine.com | Health Care|Health Care Information Technology|Hospitals | 4e+06               | operating | USA            | WI           | Madison     | Madison       | 2                |\n",
            "| One Block Off the Grid (1BOG) | http://1bog.org            | Clean Technology|Residential Solar                       | 5e+06               | closed    | USA            | CA           | SF Bay Area | San Francisco | 1                |\n",
            "\n",
            "Super-cleaned data saved to: super_cleaned_startup_data_for_prediction.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "FILE_TO_LOAD = 'super_cleaned_startup_data_for_prediction.csv'\n",
        "\n",
        "try:\n",
        "    df_cleaned = pd.read_csv(FILE_TO_LOAD)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{FILE_TO_LOAD}' was not found. Please run 'cb_insights_scraper.py' first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading {FILE_TO_LOAD}: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Starting shape for advanced cleaning: {df_cleaned.shape}\")\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "name_mask = df_cleaned['name'].astype(str).str.match(r'^[a-zA-Z]')\n",
        "df_cleaned = df_cleaned[name_mask]\n",
        "print(f\"Shape after removing names starting with non-letters: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "numeric_state_mask = df_cleaned['state_code'].astype(str).str.isnumeric()\n",
        "\n",
        "df_cleaned = df_cleaned[~numeric_state_mask]\n",
        "print(f\"Shape after removing numeric state codes: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "single_letter_mask = df_cleaned['name'].astype(str).str.len() == 1\n",
        "df_cleaned = df_cleaned[~single_letter_mask]\n",
        "print(f\"Shape after removing single-letter names: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "\n",
        "\n",
        "initial_shape = df_cleaned.shape[0]\n",
        "df_cleaned['funding_total_usd'] = pd.to_numeric(\n",
        "    df_cleaned['funding_total_usd'],\n",
        "    errors='coerce'\n",
        ")\n",
        "# Now drop the rows that became NaN due to the coercion\n",
        "df_cleaned.dropna(subset=['funding_total_usd'], inplace=True)\n",
        "print(f\"Shape after validating/cleaning funding values: {df_cleaned.shape} ({initial_shape - df_cleaned.shape[0]} rows removed)\")\n",
        "\n",
        "\n",
        "# --- 5. Reduce dataset to the 2000 most 'valid' (first) rows ---\n",
        "TARGET_ROWS = 3000\n",
        "initial_shape_trunc = df_cleaned.shape[0]\n",
        "\n",
        "if initial_shape_trunc > TARGET_ROWS:\n",
        "    df_cleaned = df_cleaned.head(TARGET_ROWS)\n",
        "    print(f\"Dataset truncated to the top {TARGET_ROWS} rows.\")\n",
        "elif initial_shape_trunc < TARGET_ROWS:\n",
        "    print(f\"Warning: Only {initial_shape_trunc} rows remain after cleaning. No truncation performed.\")\n",
        "else:\n",
        "    print(f\"Dataset size is exactly {TARGET_ROWS} rows after cleaning. No truncation needed.\")\n",
        "\n",
        "\n",
        "# --- Final Output and Save ---\n",
        "\n",
        "print(\"\\n--- Final Cleaned Data Summary ---\")\n",
        "print(f\"Final shape of the super-cleaned DataFrame: {df_cleaned.shape}\")\n",
        "\n",
        "# Display the first few rows of the super-cleaned data\n",
        "print(\"\\nFirst 5 rows of the super-cleaned data:\")\n",
        "print(df_cleaned.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Save the super-cleaned DataFrame to a new CSV file\n",
        "SUPER_CLEANED_FILE_NAME = 'super_cleaned_startup_data_for_prediction.csv'\n",
        "# NOTE: This line is redundant since the file has the same name as the input,\n",
        "# but we keep it here to ensure the final, cleaned data is saved.\n",
        "df_cleaned.to_csv(SUPER_CLEANED_FILE_NAME, index=False)\n",
        "print(f\"\\nSuper-cleaned data saved to: {SUPER_CLEANED_FILE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bme_6vfrOyW0",
        "outputId": "1313ff1f-bfa7-4c6b-b5fe-e00af3d719e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "df_cleaned.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nk3iLlHUVLm",
        "outputId": "8f83702d-f9b3-4c69-fac4-c3fedb724210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            name                homepage_url  \\\n",
              "0         Ondine Biomedical Inc.        http://ondinebio.com   \n",
              "2                       One Inc.          http://whatis1.com   \n",
              "3                        ZenChef      http://zenchef.com/en/   \n",
              "4                          Redox  http://www.redoxengine.com   \n",
              "5  One Block Off the Grid (1BOG)             http://1bog.org   \n",
              "\n",
              "                                       category_list  funding_total_usd  \\\n",
              "0                                      Biotechnology       7.628510e+05   \n",
              "2                                             Mobile       1.150050e+06   \n",
              "3                       Local Businesses|Restaurants       1.066177e+07   \n",
              "4  Health Care|Health Care Information Technology...       4.000000e+06   \n",
              "5                 Clean Technology|Residential Solar       5.000000e+06   \n",
              "\n",
              "      status country_code state_code       region           city  \\\n",
              "0  operating          CAN         BC    Vancouver      Vancouver   \n",
              "2  operating          USA         CA  SF Bay Area  San Francisco   \n",
              "3  operating          FRA         A8        Paris          Paris   \n",
              "4  operating          USA         WI      Madison        Madison   \n",
              "5     closed          USA         CA  SF Bay Area  San Francisco   \n",
              "\n",
              "   funding_rounds  \n",
              "0               2  \n",
              "2               3  \n",
              "3               5  \n",
              "4               2  \n",
              "5               1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-655ce72b-7e08-4786-a9aa-128e90cb1fb3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>homepage_url</th>\n",
              "      <th>category_list</th>\n",
              "      <th>funding_total_usd</th>\n",
              "      <th>status</th>\n",
              "      <th>country_code</th>\n",
              "      <th>state_code</th>\n",
              "      <th>region</th>\n",
              "      <th>city</th>\n",
              "      <th>funding_rounds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ondine Biomedical Inc.</td>\n",
              "      <td>http://ondinebio.com</td>\n",
              "      <td>Biotechnology</td>\n",
              "      <td>7.628510e+05</td>\n",
              "      <td>operating</td>\n",
              "      <td>CAN</td>\n",
              "      <td>BC</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>Vancouver</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>One Inc.</td>\n",
              "      <td>http://whatis1.com</td>\n",
              "      <td>Mobile</td>\n",
              "      <td>1.150050e+06</td>\n",
              "      <td>operating</td>\n",
              "      <td>USA</td>\n",
              "      <td>CA</td>\n",
              "      <td>SF Bay Area</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ZenChef</td>\n",
              "      <td>http://zenchef.com/en/</td>\n",
              "      <td>Local Businesses|Restaurants</td>\n",
              "      <td>1.066177e+07</td>\n",
              "      <td>operating</td>\n",
              "      <td>FRA</td>\n",
              "      <td>A8</td>\n",
              "      <td>Paris</td>\n",
              "      <td>Paris</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Redox</td>\n",
              "      <td>http://www.redoxengine.com</td>\n",
              "      <td>Health Care|Health Care Information Technology...</td>\n",
              "      <td>4.000000e+06</td>\n",
              "      <td>operating</td>\n",
              "      <td>USA</td>\n",
              "      <td>WI</td>\n",
              "      <td>Madison</td>\n",
              "      <td>Madison</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>One Block Off the Grid (1BOG)</td>\n",
              "      <td>http://1bog.org</td>\n",
              "      <td>Clean Technology|Residential Solar</td>\n",
              "      <td>5.000000e+06</td>\n",
              "      <td>closed</td>\n",
              "      <td>USA</td>\n",
              "      <td>CA</td>\n",
              "      <td>SF Bay Area</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-655ce72b-7e08-4786-a9aa-128e90cb1fb3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-655ce72b-7e08-4786-a9aa-128e90cb1fb3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-655ce72b-7e08-4786-a9aa-128e90cb1fb3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3fb6c4b5-050a-48ab-a0d3-245372201837\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fb6c4b5-050a-48ab-a0d3-245372201837')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3fb6c4b5-050a-48ab-a0d3-245372201837 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_combined",
              "summary": "{\n  \"name\": \"df_combined\",\n  \"rows\": 4279,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4279,\n        \"samples\": [\n          \"AlpineReplay\",\n          \"AtheroMed\",\n          \"Teya\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"homepage_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4278,\n        \"samples\": [\n          \"http://www.aductions.com\",\n          \"http://www.adcentricity.com\",\n          \"http://avantbio.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_list\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1431,\n        \"samples\": [\n          \"Education|Online Education|Services\",\n          \"Health Care|Innovation Management\",\n          \"Consumer Electronics|Human Computer Interaction|Internet of Things|Music Services|New Technologies|Wearables\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"funding_total_usd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56453529.25595024,\n        \"min\": 88.78,\n        \"max\": 2394820000.0,\n        \"num_unique_values\": 1838,\n        \"samples\": [\n          28561000.0,\n          63388808.0,\n          3301958.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"closed\",\n          \"ipo\",\n          \"operating\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country_code\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 317,\n        \"samples\": [\n          \"Faridabad\",\n          \"Lausanne\",\n          \"Roseville\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state_code\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 134,\n        \"samples\": [\n          \"Media & Entertainment\",\n          \"B9\",\n          \"C7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 260,\n        \"samples\": [\n          \"Toronto\",\n          \"FL - Other\",\n          \"Charleston, South Carolina\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"city\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 764,\n        \"samples\": [\n          \"Mclean\",\n          \"Richland\",\n          \"Davao City\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"funding_rounds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          12,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "df_combined.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmYedEyVbEbx"
      },
      "source": [
        "#**Data Preprocessing and EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGgR_GLFYylK",
        "outputId": "53309dec-7bac-4e66-dbb1-1ae842354de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data for preprocessing. Initial shape: (3000, 10)\n",
            "\n",
            "--- 2. Data Preprocessing (Cleaning and Type Conversion) ---\n",
            "Rows removed due to missing Funding/Rounds: 0\n",
            "Data shape after cleaning and preparation: (3000, 10)\n",
            "\n",
            "--- 3.1 Descriptive Statistics (Numeric Features in INR) ---\n",
            "|       | funding_total_usd   | funding_rounds   |\n",
            "|:------|:--------------------|:-----------------|\n",
            "| count | 3000                | 3000             |\n",
            "| mean  | 2.01838e+07         | 2.14833          |\n",
            "| std   | 6.6516e+07          | 1.71191          |\n",
            "| min   | 100                 | 1                |\n",
            "| 25%   | 669238              | 1                |\n",
            "| 50%   | 3.459e+06           | 1                |\n",
            "| 75%   | 1.63568e+07         | 3                |\n",
            "| max   | 2.39482e+09         | 15               |\n",
            "\n",
            "--- 3.2 Categorical Analysis: Top 5 Countries and Industries ---\n",
            "Top 5 Countries:\n",
            "| country_code   | count   |\n",
            "|:---------------|:--------|\n",
            "| USA            | 2568    |\n",
            "| GBR            | 211     |\n",
            "| CAN            | 108     |\n",
            "| FRA            | 82      |\n",
            "| San Francisco  | 8       |\n",
            "\n",
            "Top 5 Industries (Category List):\n",
            "| category_list       | count   |\n",
            "|:--------------------|:--------|\n",
            "| Biotechnology       | 375     |\n",
            "| Software            | 223     |\n",
            "| Health Care         | 83      |\n",
            "| Hardware + Software | 72      |\n",
            "| Clean Technology    | 62      |\n",
            "\n",
            "--- 3.3 Creating Binary Target: is_highly_funded ---\n",
            "Median Funding Total (in INR) used as threshold: 3,459,000\n",
            "Target distribution: 1 (Highly Funded) count: 1500 / Total rows: 3000\n",
            "New binary column 'is_highly_funded' added to the DataFrame.\n",
            "\n",
            "Prepared data saved to: prepared_for_model.csv (Ready for Feature Engineering)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2445352745.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['country_code'].fillna('Unknown Country', inplace=True)\n",
            "/tmp/ipython-input-2445352745.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['category_list'].fillna('Unknown Industry', inplace=True)\n",
            "/tmp/ipython-input-2445352745.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['region'].fillna('Unknown Region', inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the file name used by the scraper and cleaner scripts\n",
        "FILE_NAME = 'super_cleaned_startup_data_for_prediction.csv'\n",
        "\n",
        "# --- 1. Load the Super Cleaned Data ---\n",
        "try:\n",
        "    df = pd.read_csv(FILE_NAME)\n",
        "    print(f\"Successfully loaded data for preprocessing. Initial shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The file '{FILE_NAME}' was not found.\")\n",
        "    print(\"Please ensure you have run 'cb_insights_scraper.py' to generate the data.\")\n",
        "    # Exit gracefully if the critical file is missing\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. Data Preprocessing (Cleaning and Type Conversion) ---\n",
        "\n",
        "print(\"\\n--- 2. Data Preprocessing (Cleaning and Type Conversion) ---\")\n",
        "\n",
        "# The goal here is to make sure our columns are ready for math or grouping.\n",
        "\n",
        "# 2.1 Type Conversion and Missing Data (NaN) Handling for Numeric Features\n",
        "# We force these columns to be numbers. If the conversion fails (e.g., finding text),\n",
        "# it replaces that value with NaN (Not a Number).\n",
        "df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')\n",
        "df['funding_rounds'] = pd.to_numeric(df['funding_rounds'], errors='coerce')\n",
        "\n",
        "# 2.2 Missing Data Handling for Text (Categorical) Features\n",
        "# For text columns, we replace any missing values (NaN) with 'Unknown'\n",
        "# so they can be grouped correctly later.\n",
        "df['country_code'].fillna('Unknown Country', inplace=True)\n",
        "df['category_list'].fillna('Unknown Industry', inplace=True)\n",
        "df['region'].fillna('Unknown Region', inplace=True)\n",
        "\n",
        "# 2.3 Dropping Critical Missing Rows\n",
        "# We must remove any rows that still have missing values in our key number columns\n",
        "# (funding and rounds) because we can't do math or prediction without them.\n",
        "initial_rows = df.shape[0]\n",
        "df.dropna(subset=['funding_total_usd', 'funding_rounds'], inplace=True)\n",
        "rows_removed = initial_rows - df.shape[0]\n",
        "\n",
        "print(f\"Rows removed due to missing Funding/Rounds: {rows_removed}\")\n",
        "print(f\"Data shape after cleaning and preparation: {df.shape}\")\n",
        "\n",
        "\n",
        "# --- 3. Exploratory Data Analysis (EDA) ---\n",
        "\n",
        "print(\"\\n--- 3.1 Descriptive Statistics (Numeric Features in INR) ---\")\n",
        "# This gives us a quick summary (count, average, min, max, median) of our key numbers.\n",
        "print(df[['funding_total_usd', 'funding_rounds']].describe().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\n--- 3.2 Categorical Analysis: Top 5 Countries and Industries ---\")\n",
        "# This shows us which categories appear most often in the dataset.\n",
        "print(\"Top 5 Countries:\")\n",
        "print(df['country_code'].value_counts().head(5).to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\nTop 5 Industries (Category List):\")\n",
        "print(df['category_list'].value_counts().head(5).to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "# --- 3.3 CRITICAL STEP: Creating the Binary Target Variable (0 or 1) ---\n",
        "\n",
        "print(\"\\n--- 3.3 Creating Binary Target: is_highly_funded ---\")\n",
        "# This is our 'Success' score for prediction. We are turning a continuous number (funding)\n",
        "# into a simple YES (1) or NO (0) question for the model to learn.\n",
        "\n",
        "# 1. Calculate the middle (median) funding amount across all companies.\n",
        "median_funding = df['funding_total_usd'].median()\n",
        "\n",
        "# 2. Create the binary target column:\n",
        "# 1 if the company's funding is ABOVE the median, 0 if it is at or BELOW the median.\n",
        "df['is_highly_funded'] = (df['funding_total_usd'] > median_funding).astype(int)\n",
        "\n",
        "print(f\"Median Funding Total (in INR) used as threshold: {median_funding:,.0f}\")\n",
        "print(f\"Target distribution: 1 (Highly Funded) count: {df['is_highly_funded'].sum()} / Total rows: {df.shape[0]}\")\n",
        "print(\"New binary column 'is_highly_funded' added to the DataFrame.\")\n",
        "\n",
        "# --- 4. Prepare for Next Step (Feature Engineering) ---\n",
        "# Save the new DataFrame with the target variable, ready for the machine learning model.\n",
        "df.to_csv('prepared_for_model.csv', index=False)\n",
        "print(\"\\nPrepared data saved to: prepared_for_model.csv (Ready for Feature Engineering)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZyRoidla7pT"
      },
      "source": [
        "#**Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4CD3bjqbYNl",
        "outputId": "17ef311a-5952-4362-e5d9-59df7d767322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data for modeling. Initial shape: (3000, 11)\n",
            "\n",
            "--- 1.5. Applying Domain Constraints ---\n",
            "Applied constraint: Set 'success' to 0 for 216 rows where status is 'closed'.\n",
            "Target column renamed to 'success'.\n",
            "\n",
            "Features selected: ['category_list', 'country_code', 'state_code', 'region', 'city', 'funding_rounds', 'status']\n",
            "\n",
            "--- 3. Feature Engineering: One-Hot Encoding (Converting Text to Numbers) ---\n",
            "Original feature columns: 7\n",
            "New encoded feature columns (numbers only): 2557\n",
            "Data is now numerical and ready for the model.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Libraries for splitting data, training the model, and checking performance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the file name from the previous preprocessing step\n",
        "FILE_NAME = 'prepared_for_model.csv'\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    df = pd.read_csv(FILE_NAME)\n",
        "    print(f\"Successfully loaded data for modeling. Initial shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The file '{FILE_NAME}' was not found.\")\n",
        "    print(\"Please ensure you have run 'data_preprocessing_eda.py' first.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 1.5. Apply Domain Constraint and Rename Target ---\n",
        "\n",
        "# Rename the target column to 'success' as requested\n",
        "df.rename(columns={'is_highly_funded': 'success'}, inplace=True)\n",
        "\n",
        "print(\"\\n--- 1.5. Applying Domain Constraints ---\")\n",
        "# Constraint: A closed company cannot be considered successful (a positive prediction).\n",
        "# We explicitly set the 'success' score to 0 for any row where status is 'closed'.\n",
        "closed_count = df[df['status'] == 'closed'].shape[0]\n",
        "df.loc[df['status'] == 'closed', 'success'] = 0\n",
        "print(f\"Applied constraint: Set 'success' to 0 for {closed_count} rows where status is 'closed'.\")\n",
        "print(f\"Target column renamed to 'success'.\")\n",
        "\n",
        "\n",
        "# --- 2. Feature Selection ---\n",
        "\n",
        "# X (Features): The columns the model will use to guess the outcome.\n",
        "# We choose columns available to a young startup (location, industry, rounds) AND status.\n",
        "feature_cols = [\n",
        "    'category_list', 'country_code', 'state_code', 'region', 'city', 'funding_rounds', 'status'\n",
        "]\n",
        "X = df[feature_cols]\n",
        "\n",
        "# y (Target): The column the model is trying to predict (our 0 or 1 success score).\n",
        "# This is now the 'success' column.\n",
        "y = df['success']\n",
        "\n",
        "print(f\"\\nFeatures selected: {feature_cols}\")\n",
        "\n",
        "\n",
        "# --- 3. Feature Engineering (One-Hot Encoding) ---\n",
        "\n",
        "print(\"\\n--- 3. Feature Engineering: One-Hot Encoding (Converting Text to Numbers) ---\")\n",
        "# Machine learning models only understand numbers. This step creates a new column\n",
        "# (with 0s and 1s) for every unique text value in the original columns.\n",
        "# Example: 'country_code' turns into 'country_code_USA', 'country_code_CHN', etc.\n",
        "X_encoded = pd.get_dummies(X, columns=feature_cols, drop_first=True)\n",
        "\n",
        "print(f\"Original feature columns: {len(feature_cols)}\")\n",
        "print(f\"New encoded feature columns (numbers only): {X_encoded.shape[1]}\")\n",
        "print(\"Data is now numerical and ready for the model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "so5yGnyV1Kpu",
        "outputId": "07471c55-a2dc-4750-c213-0c4e54c037d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set size: 2400 rows (80% of data)\n",
            "Testing set size: 600 rows (20% of data)\n",
            "\n",
            "--- 5. Hyperparameter Tuning using GridSearchCV ---\n",
            "This step searches for the optimal parameters to maximize the F1-Score...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 4. Data Splitting (Train/Test Split) ---\n",
        "\n",
        "# We split the data so the model can learn from one part (Train) and be tested\n",
        "# on the other part (Test) to see if it generalizes to new data.\n",
        "# Libraries for splitting data, training the model, and checking performance\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier # Added this import\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape[0]} rows (80% of data)\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} rows (20% of data)\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 5. Hyperparameter Tuning (Finding the Best Random Forest Settings) ---\n",
        "\n",
        "print(\"\\n--- 5. Hyperparameter Tuning using GridSearchCV ---\")\n",
        "print(\"This step searches for the optimal parameters to maximize the F1-Score...\")\n",
        "#\n",
        "\n",
        "# Define the model to tune (starting with a balanced class weight)\n",
        "rf_base = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Define the grid of parameters to test. We try different numbers of trees (n_estimators)\n",
        "# and different maximum depths of those trees (max_depth).\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200], # Number of trees in the forest\n",
        "    'max_depth': [5, 10, None] # Maximum depth of each tree (None means unlimited)\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to automatically test every combination in the grid.\n",
        "# We set scoring='f1' to ensure the model optimizes for the F1-Score (our primary success metric).\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_base,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3, # Use 3-fold cross-validation\n",
        "    verbose=0,\n",
        "    n_jobs=-1 # Use all available cores for speed\n",
        ")\n",
        "\n",
        "# Train the grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# The best model found during the search\n",
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Hyperparameter tuning complete.\")\n",
        "print(f\"BEST parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- 5. Model Training: Two Classifiers ---\")\n",
        "\n",
        "# Model A: Random Forest (Excellent for complex, mixed data)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Random Forest Model training complete.\")\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "rf_y_pred = rf_model.predict(X_test)\n",
        "rf_f1 = f1_score(y_test, rf_y_pred)\n",
        "print(f\"Random Forest F1-Score: {rf_f1:.4f}\")\n",
        "\n",
        "# Model B: K-Nearest Neighbors (KNN - Simple, distance-based model)\n",
        "# We use a neighbor count of 5 as a starting point.\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "print(\"K-Nearest Neighbors (KNN) Model training complete.\")\n",
        "\n",
        "# Evaluate KNN model\n",
        "knn_y_pred = knn_model.predict(X_test)\n",
        "knn_f1 = f1_score(y_test, knn_y_pred)\n",
        "print(f\"K-Nearest Neighbors (KNN) F1-Score: {knn_f1:.4f}\")\n",
        "\n",
        "\n",
        "# --- 6.3 Final Model Comparison and Selection ---\n",
        "print(\"\\n--- Final Model Selection ---\")\n",
        "if rf_f1 >= knn_f1:\n",
        "    best_model_name = \"Random Forest Classifier\"\n",
        "    best_model = rf_model\n",
        "    y_pred = rf_y_pred\n",
        "    y_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "    best_f1 = rf_f1\n",
        "else:\n",
        "    best_model_name = \"K-Nearest Neighbors (KNN)\"\n",
        "    best_model = knn_model\n",
        "    y_pred = knn_y_pred\n",
        "    y_proba = knn_model.predict_proba(X_test)[:, 1]\n",
        "    best_f1 = knn_f1\n",
        "\n",
        "print(f\"Based on the F1-Score, the BEST model is the: {best_model_name} (F1: {best_f1:.4f})\")\n",
        "print(\"We prioritize F1-Score over simple Accuracy because it ensures we find true winners (Recall) without making too many false positive predictions (Precision).\")\n",
        "\n",
        "# Assign the best model to 'model' for consistency with subsequent cells\n",
        "model = best_model\n",
        "\n",
        "# --- 7. Feature Importance (Why the model made the decision) ---\n",
        "# Feature importance is only available for the Random Forest model.\n",
        "print(\"\\n--- 7. Feature Importance (Top 10 from Random Forest if selected) ---\")\n",
        "if best_model_name == \"Random Forest Classifier\":\n",
        "    feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns)\n",
        "    top_features = feature_importances.nlargest(10)\n",
        "    print(\"Which input factors are most important for predicting 'Highly Funded' (1) (from Random Forest):\")\n",
        "    print(top_features.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "else:\n",
        "    print(\"Feature importance is not directly available for K-Nearest Neighbors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzv9O4sy7uZT"
      },
      "source": [
        "#F1-score, confusion matrix (for classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W3B8fzAT1dPz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 6.1 F1-Score and Classification Report\n",
        "# F1-Score measures the balance between Precision (how accurate is the '1' prediction?) and\n",
        "# Recall (how many true '1's did the model find?).\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model F1-Score on Test Data: {f1:.4f} (Closer to 1.0 is better)\")\n",
        "print(\"\\nClassification Report (Focus on F1-score for class 1):\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Moderately Funded (0)', 'Highly Funded (1)']))\n",
        "\n",
        "\n",
        "# 6.2 Confusion Matrix\n",
        "# This shows exactly where the model was right and wrong on the test data.\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "# Unpack the matrix components: True Negatives, False Positives, False Negatives, True Positives\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Data):\")\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Interpreting the Matrix:\")\n",
        "print(f\"| True Negatives (Correctly guessed 0s): {tn:<10} | False Positives (Wrongly guessed 1s): {fp:<10} |\")\n",
        "print(f\"| False Negatives (Missed 1s): {fn:<10} | True Positives (Correctly guessed 1s): {tp:<10} |\")\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# --- 7. Feature Importance (Why the model made the decision) ---\n",
        "print(\"\\n--- 7. Feature Importance (Top 10) ---\")\n",
        "\n",
        "# Feature importance tells us which input factors (Country, City, Round number, Industry)\n",
        "# the model considered most important when making a prediction.\n",
        "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns)\n",
        "top_features = feature_importances.nlargest(10)\n",
        "\n",
        "print(\"Most important for predicting success (1):\")\n",
        "print(top_features.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "authorship_tag": "ABX9TyNhKdc+bItYu9py5a49tkFr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}